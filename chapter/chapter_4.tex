\chapter{Methodology}
\section{Block Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/BlockDiagram.png}
	\caption{Block Diagram}
	\label{fig:block-diagram}
\end{figure}
This block diagram illustrates the system architecture of \textbf{SiteCortex}, detailing the interaction between the frontend user interface, the backend services, and the core data and model assets.

\begin{itemize}
    \item \textbf{User Interface (Frontend):} The user's journey begins here. They input their unique \textbf{Business Profile} (e.g., \textit{`Quiet Study Hub' vibe}, budget, rent) into the UI. The frontend is responsible for rendering the \textbf{Personalized Opportunity Zone Heatmap} and, upon user selection of a point, displaying the rich, multi-faceted \textbf{Detailed Analysis Dashboard}.

    \item \textbf{Backend Services (API Gateway):} Acting as the central nervous system, the FastAPI backend orchestrates the entire process. When it receives the user's profile, it tasks the \textbf{Property Listing Filter} to query the database for relevant "To-Let" properties. Simultaneously, its \textbf{Dynamic Feature Analyzer} queries the geospatial databases (Competitors, POIs, Demographics) to generate the data needed for the personalized heatmap. When a user requests a deep analysis, the Feature Analyzer gathers all features for that specific point and passes them to the \textbf{Predictor Service}.

    \item \textbf{Data \& Model Assets:} These are the foundational pillars of the system's intelligence.
    \begin{itemize}
        \item \textbf{The Database Cluster:} A PostgreSQL database with the PostGIS extension, containing separate, optimized tables for \texttt{Listings}, \texttt{Competitors} (Businesses), \texttt{Demographics} (Neighborhoods), and \texttt{Points of Interest}.
        \item \textbf{The Trained Model:} A pre-trained \textbf{XGBoost model file}. The Predictor Service loads this model to calculate the final, personalized \textbf{Suitability Score} for the user's specific business profile at the selected location.
    \end{itemize}
\end{itemize}
\pagebreak
\section{Usecase Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/UsecaseDiagram.png}
	\caption{Usecase Diagram}
	\label{fig:Usecase-diagram}
\end{figure}\
\noindent
\par This use case diagram illustrates the interactions between the system's two primary actors---the \textbf{Entrepreneur} (end-user) and the \textbf{Project Admin}---and the core functionalities of the SiteCortex platform.

\begin{itemize}
    \item \textbf{Entrepreneur:} This actor represents the primary user, a small business owner. Their interactions are focused on the decision-making journey and include use cases such as:
    \begin{itemize}
        \item \texttt{Define Business Profile}: The crucial first step where the user inputs their unique strategy and requirements to personalize the analysis.
        \item \texttt{Explore Personalized Map \& Listings}: Interacting with the custom-generated heatmap and available properties.
        \item \texttt{Select point for Deep analysis}: The central action that triggers the system's full analytical power.
        \item \texttt{Compare Saved Locations} and \texttt{Generate PDF Report}: Tools to support the final decision-making process.
    \end{itemize}

    \item \textbf{Project Admin:} This actor is responsible for maintaining the system's intelligence and data integrity. Their tasks include:
    \begin{itemize}
        \item \texttt{Manage Geographic Data} and \texttt{Manage 'To-Let' Listings}: Keeping the system's foundational data accurate and up-to-date.
        \item \texttt{Train \& Validate ML Model}: Running the offline machine learning pipeline to update the predictive model.
        \item \texttt{Monitor System Analytics}: Observing system performance and usage.
    \end{itemize}

    \item \textbf{Core Relationships:} The diagram critically shows that the \texttt{Select point for Deep analysis} use case \textbf{\texttt{<<includes>>}} a bundle of sub-processes: generating the score, the XAI breakdown, and analyzing the trade area and competitors. This demonstrates how a single user action initiates a comprehensive, multi-faceted analysis in the backend.
\end{itemize}
\pagebreak
% \setlength{\parindent}{15pt}
\section{Zero Context level Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/ContextDiagram.png}
	\caption{0-level Context Diagram}
	\label{fig:Context-level-diagram}
\end{figure}
\noindent
\par This 0-level context diagram provides a high-level overview of the SiteCortex platform, illustrating its boundaries and its interactions with external entities. It shows the primary data flows into and out of the system without detailing the internal processes.

\begin{description}
    \item[Central System:] The \textbf{SiteCortex: Location Intelligence Platform} is the core process that transforms raw data and user requirements into actionable business insights.

    \item[External Entities:]
    \begin{itemize}
        \item \textbf{Entrepreneur:} The primary user who initiates the analysis and receives the final intelligence.
        \item \textbf{Project Admin:} The team responsible for curating and inputting verified ground-truth data into the system.
        \item \textbf{Public Geospatial Data Sources:} External sources like OpenStreetMap that provide foundational data (base maps, POIs, road networks).
        \item \textbf{Real Estate Data Sources:} External sources for property listings and pricing data, which are used for both the property discovery feature and for deriving demographic proxies.
        \item \textbf{OpenRouteService API:} An external, third-party service that the system calls upon to perform the complex task of calculating realistic trade area isochrones.
    \end{itemize}

    \item[Primary Data Flows:] The diagram highlights the interactive, multi-step dialogue between the Entrepreneur and the system. The user first sends their \texttt{Business Profile}, receives a \texttt{Personalized Heatmap}, then sends a \texttt{Selected Location for Analysis} to receive the final \texttt{Detailed Analysis Dashboard \& Report}.
\end{description}
\pagebreak
\section{Level-1 Context Diagram}
\begin{figure}[h!]
	\centering
	% \includegraphics[width=\textwidth]{img/Level1ContextDiagram.png}
	\caption{Level-1 Context Diagram}
	\label{fig:level1-context-diagram}
\end{figure}
\noindent
The Level-1 Context Diagram provides a decomposed view of the SiteCortex system, expanding upon the 0-level context by detailing the system’s internal functional processes and their interactions with external entities and data stores.

\begin{itemize}
	\item \textbf{Core Processes:}
	The SiteCortex platform is divided into four primary internal processes:
	\begin{enumerate}
		\item \textit{Handle API Requests}: Serves as the central coordination layer, managing incoming user requests, routing data flows, and aggregating responses.
		\item \textit{Calculate Location Features}: Responsible for deriving spatial and contextual features such as competitor density, POI accessibility, and demographic indicators.
		\item \textit{Predict Suitability Score}: Executes machine learning inference using the trained XGBoost model to compute the final suitability score.
		\item \textit{Filter Property Listings}: Filters available “To-Let” properties based on user-defined requirements such as budget, area, and location constraints.
	\end{enumerate}

	\item \textbf{Data Stores:}
	Each process interacts with specialized databases optimized for specific data types:
	\begin{itemize}
		\item \textbf{Listings Database:} Stores property listings and rental information.
		\item \textbf{Competitor Database:} Contains data about existing businesses in the surrounding area.
		\item \textbf{Demographics Database:} Holds neighborhood-level population and socio-economic data.
		\item \textbf{POI Database:} Stores geospatial information about nearby amenities and landmarks.
		\item \textbf{Trained Model Repository:} Stores the pre-trained XGBoost model used for prediction.
	\end{itemize}

	\item \textbf{Data Flow Overview:}
	User requests originating from the UI Frontend enter the system through the API handler. Feature computation processes query the relevant databases and return calculated features. These features are then passed to the prediction process, which generates suitability scores and heatmap values. The filtered listings, scores, and visual analytics are consolidated and returned to the frontend for presentation.
\end{itemize}

This Level-1 Context Diagram emphasizes the modular, service-oriented architecture of SiteCortex, illustrating how independent yet interconnected components collaborate to transform raw geospatial data into personalized, decision-support intelligence for entrepreneurs.

\section{Workflow Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/WorkflowDiagram.png}
	\caption{Workflow Diagram}
	\label{fig:Workflow-diagram}
\end{figure}
\noindent
\par This diagram illustrates the two primary and distinct workflows that define the entire SiteCortex project: the offline data science pipeline that builds the system's intelligence, and the online user journey that consumes it.

\begin{itemize}
    \item \textbf{Admin Workflow (Right Side):} This represents the foundational, \textbf{offline process} performed by the project team. It begins with collecting multi-layered data (geospatial, demographic, listings) and populating the relational database. This structured data is then fed into an ML Pipeline to engineer features and train the XGBoost model. The two critical outputs of this workflow are the system's core assets: the populated \textbf{Relational Database} and the final \textbf{Trained Model} file.

    \item \textbf{Business Owner's Workflow (Left Side):} This represents the \textbf{real-time, online journey} for the end-user. The process is initiated when the user defines their requirements. The backend uses these to filter listings and generate a personalized heatmap. After the user explores the map and selects a point, the backend performs the full analysis by calculating the score, gathering insights, and returning the complete package to the user's detailed dashboard.

    \item \textbf{The Bridge:} The diagram critically shows the connection between the two workflows. The assets created by the offline Admin Workflow are the essential inputs that power the live Business Owner Workflow. The \textbf{Relational Database} is actively queried for generating the heatmap and gathering competitor data, while the \textbf{Trained Model} is loaded on-demand to calculate the personalized score, seamlessly connecting the data science foundation to the interactive user application.
\end{itemize}
% \noindent
%     The development of the predictive site selection platform will follow a systematic methodology grounded in geospatial data analysis and supervised machine learning. The project is designed to transform raw geographic data into actionable business intelligence, providing a data-driven forecast of a new retail location's potential success. The workflow encompasses geospatial data aggregation, feature engineering, predictive model training, and deployment via an interactive web application.

\section{Sequence Diagram}
\begin{figure}[h!]
	\centering
	% \includegraphics[width=\textwidth]{img/SequenceDiagram.png}
	\caption{Sequence Diagram}
	\label{fig:sequence-diagram}
\end{figure}
\noindent
This sequence diagram illustrates the chronological interaction between the user, frontend interface, backend services, databases, and the machine learning model during a complete analysis cycle in the SiteCortex platform. It highlights how a user’s request is transformed into actionable location intelligence through coordinated backend operations.

\begin{itemize}
	\item \textbf{User \& Frontend Interaction:}
	The workflow begins when the user accesses the system through the UI Frontend. Initially, the frontend requests nearby Points of Interest (POIs) data to render an interactive base map. The user then enters a specific location and defines their business preferences, such as business type and constraints.

	\item \textbf{API Request Handling:}
	Upon submission, the UI sends an analysis request to the backend \textit{Handle API Requests} service. This component acts as the orchestrator, managing communication between feature computation services, databases, and the prediction engine.

	\item \textbf{Feature Calculation Process:}
	The API service requests location-based features from the \textit{Calculate Location Features} module. This module queries multiple data sources—including POI data, competitor information, and demographic datasets—from the database cluster. After aggregating and processing this information, it returns a structured feature vector representing the selected location.

	\item \textbf{Suitability Prediction:}
	The computed feature vector is forwarded to the \textit{Predict Suitability Score} service. This service dynamically loads the pre-trained XGBoost model and performs inference to generate a personalized suitability score. In parallel, supporting spatial outputs such as heatmap intensity values are generated.

	\item \textbf{Result Delivery:}
	The predicted score and heatmap data are returned to the API service, which then sends the consolidated results back to the UI Frontend. Finally, the frontend displays the suitability score, visual heatmap, and detailed insights to the user, completing the interaction loop.
\end{itemize}

This sequence diagram clearly demonstrates how SiteCortex integrates real-time user inputs with geospatial analytics and machine learning inference to deliver instant, data-driven business location recommendations.
\subsection{Geospatial Data Collection and Aggregation}

The project's foundation is a high-quality, multi-layered geographic dataset. This "Master Dataset" will be built by aggregating data from public online sources and manual field verification within the core commercial area of Bhaktapur. The primary tool for this phase will be the open-source Geographic Information System (GIS) software, \texttt{QGIS}. Data is collected and organized into a relational structure with four primary layers:

\begin{itemize}
	\item \textbf{Businesses Layer (Competitors):} Locations of all existing cafes, sourced initially from OpenStreetMap (OSM) and then verified and augmented through manual "ground-truthing" to ensure accuracy and to collect ground-truth data for training the model's target variable.
	\item \textbf{Points of Interest (POIs) Layer:} Locations that attract foot traffic (e.g., schools, temples, offices, bus stops), sourced from OSM and manually verified. This layer acts as a source for "people magnets."
    \item \textbf{Neighborhoods Layer:} GIS polygons defining different zones across Bhaktapur. This layer is enriched with \textbf{demographic proxies} (e.g., average rent per square foot, student service density) derived from scraped real estate data and POI density analysis.
    \item \textbf{Listings Layer:} Dynamic data of available "To-Let" commercial properties, including their location, area, and monthly rent, integrated to bridge analysis with real-world opportunities.
\end{itemize}

Each data point is stored with its precise geographic coordinates in a central PostGIS database, enabling high-performance spatial queries.

\subsection{Feature Engineering and Data Structuring}

This critical phase involves converting the raw geospatial data into a structured, numerical format suitable for machine learning. This process, known as feature engineering, is performed using GIS analysis tools within QGIS and Python libraries like \texttt{GeoPandas}. For any given point on the map, a rich set of predictive features is calculated on-the-fly:

\begin{itemize}
	\item \textbf{Competitor Density:} The number of other cafes within multiple radii (e.g., 100m, 300m), calculated using a buffer analysis.
	\item \textbf{Proximity to POIs:} The straight-line distance to the nearest "People Magnet" in various categories (e.g., nearest school, nearest bus stop).
    \item \textbf{Neighborhood-based Demographic Features:} The point inherits features from the neighborhood polygon it falls within, such as \texttt{avg\_rent\_psf} or \texttt{population\_density}. This allows the model to understand the socio-economic context of a location.
	\item \textbf{Visibility \& Accessibility Scores:} Manually assigned scores based on a location's visibility from main pathways and its road classification (e.g., Main Commercial Route, Local Alley).
\end{itemize}

Alongside these predictive features, the model is trained on a carefully defined target variable:
\begin{itemize}
	\item \textbf{Popularity Score (Target Variable):} A composite score (1-10) that serves as a proxy for business success. It is calculated for each existing cafe using publicly available data like Google Maps ratings, the number of reviews, and social media presence.
\end{itemize}

To prepare for model training and evaluation, the dataset of existing cafes is partitioned using a standard train-test-validation split: 70\% for training, 15\% for validation (hyperparameter tuning), and 15\% for final testing on unseen data.

\subsection{Personalized Predictive Modeling System}

The core of the application is a system that provides personalized, real-time predictions for new, hypothetical locations. The user interaction is designed to be intuitive and user-centric:

\begin{itemize}
    \item A user begins by defining their specific \textbf{business profile}, including strategy (e.g., 'Quiet Study Hub', 'Tourist Hotspot'), budget, and space requirements.
	\item The backend uses this profile to generate a \textbf{personalized opportunity heatmap} and filter relevant property listings.
	\item The user selects any point on the interactive map for deep analysis.
	\item The frontend sends the coordinates and the user's profile to the backend API.
	\item The backend, in real-time, calculates all engineered features for that specific point by querying the geographic database.
	\item These features are fed into the pre-trained machine learning model, which returns a \textbf{Personalized Suitability Score}.
	\item The system also provides an explainable AI \textbf{Prediction Breakdown} (powered by SHAP analysis), showing which features contributed most positively or negatively to the score, offering actionable insights.
\end{itemize}

\subsection{Predictive Engine via Supervised Learning}

The project utilizes a \textbf{Supervised Learning} regression model. This approach is ideal as we are training a model to predict a specific, known continuous value (the Popularity Score) based on a labeled dataset of existing, successful and unsuccessful cafes.

The system is modeled using the standard supervised regression framework:

\begin{table}[h!]
\centering
\begin{tabular}{|l|p{0.7\textwidth}|}
	\hline
	\textbf{ML Component}         & \textbf{Description} \\
	\hline
	\textbf{Input Features (X):}  & A vector of numerical values representing the engineered features, including competitor density, proximity to POIs, visibility scores, and inherited demographic proxies from the neighborhood. \\
	\hline
	\textbf{Target Variable (Y):} & The continuous "Popularity Score" that the model learns to predict, acting as a proxy for general business success. \\
	\hline
	\textbf{Model Architecture:}  & A gradient boosted decision tree model, specifically \textbf{XGBoost}, chosen for its high accuracy, performance, and interpretability on tabular data. \\
	\hline
	\textbf{Prediction Output:}   & A single, continuous value (e.g., 8.2) representing the \textbf{Personalized Suitability Score} for a new location, tailored to the user's business profile. \\
	\hline
\end{tabular}
\end{table}

\subsubsection{Example Input Feature Vector}
An example input vector fed to the model for a new location would be enriched with demographic data:
\begin{verbatim}
[ 2, 4, 250, 80, 3, 4, 150, 0.75 ] 
// Represents [competitors_100m, competitors_300m, 
// dist_to_school, dist_to_bus_stop, road_type_score, 
// visibility_score, neighborhood_avg_rent_psf, 
// neighborhood_student_density]
\end{verbatim}

\subsection{Model Training and Evaluation Strategy}

The training process is designed to ensure the model is both accurate and reliable.
\begin{itemize}
	\item A baseline model (e.g., Linear Regression) will be established to measure the performance uplift provided by our chosen model.
	\item The primary model, \textbf{XGBoost}, will be trained on the 70\% training dataset.
	\item Model performance will be assessed on the test set using standard regression metrics:
	      \begin{itemize}
		      \item \textbf{Mean Absolute Error (MAE):} Measures the average absolute difference between predicted and actual scores.
		      \item \textbf{Root Mean Squared Error (RMSE):} Similar to MAE but penalizes larger errors more heavily.
		      \item \textbf{R-squared (R²):} Indicates the proportion of the variance in the success score that is predictable from the input features.
	      \end{itemize}
\end{itemize}

\subsection{Database Schema and Storage}

To support real-time feature calculation and the overall application logic, all aggregated geographic data is stored in a professional-grade relational database system.
\begin{itemize}
	\item \textbf{Database Technology:} \textbf{PostgreSQL} with the \textbf{PostGIS} extension.
	\item \textbf{Why PostGIS?} This extension transforms the database into a powerful geospatial data store, providing functions to perform high-performance geographic queries directly in the database (e.g., \texttt{ST\_DWithin}, \texttt{ST\_Distance}), which is extremely efficient for our use case.
\end{itemize}

The system is built on a four-table relational schema as described in the ER diagram:
\begin{description}
    \item[\texttt{neighborhoods}] Stores GIS polygons for different zones along with their derived demographic proxies (e.g., \texttt{avg\_rent\_psf}).
    \item[\texttt{businesses}] The ground-truth data of existing competitor cafes used for training the model, including location and calculated popularity scores.
    \item[\texttt{points\_of\_interest}] Locations that attract foot traffic, such as schools, temples, and offices.
    \item[\texttt{listings}] Dynamic data about available "To-Let" properties, including area, rent, and location.
\end{description}