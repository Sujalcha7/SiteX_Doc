\chapter{Methodology}
\section{Block Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/BlockDiagram.png}
	\caption{Block Diagram}
	\label{fig:block-diagram}
\end{figure}
This block diagram illustrates the system architecture of \textbf{  SiteX}, detailing the interaction between the frontend user interface, the backend services, and the core data and model assets.

\begin{itemize}
    \item \textbf{User Interface (Frontend):} The user's journey begins here. They input their unique \textbf{Business Profile} (e.g., \textit{`Quiet Study Hub' vibe}, budget, rent) into the UI. The frontend is responsible for rendering the \textbf{Personalized Opportunity Zone Heatmap} and, upon user selection of a point, displaying the rich, multi-faceted \textbf{Detailed Analysis Dashboard}.

    \item \textbf{Backend Services (API Gateway):} Acting as the central nervous system, the FastAPI backend orchestrates the entire process. When it receives the user's profile, it tasks the \textbf{Property Listing Filter} to query the database for relevant "To-Let" properties. Simultaneously, its \textbf{Dynamic Feature Analyzer} queries the geospatial databases (Competitors, POIs, Demographics) to generate the data needed for the personalized heatmap. When a user requests a deep analysis, the Feature Analyzer gathers all features for that specific point and passes them to the \textbf{Predictor Service}.

    \item \textbf{Data \& Model Assets:} These are the foundational pillars of the system's intelligence.
    \begin{itemize}
        \item \textbf{The Database Cluster:} A PostgreSQL database with the PostGIS extension, containing separate, optimized tables for \texttt{Listings}, \texttt{Competitors} (Businesses), \texttt{Demographics} (Neighborhoods), and \texttt{Points of Interest}.
        \item \textbf{The Trained Model:} A pre-trained \textbf{XGBoost model file}. The Predictor Service loads this model to calculate the final, personalized \textbf{Suitability Score} for the user's specific business profile at the selected location.
    \end{itemize}
\end{itemize}
% \pagebreak
\section{Usecase Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/UsecaseDiagram.png}
	\caption{Usecase Diagram}
	\label{fig:Usecase-diagram}
\end{figure}\
\noindent
\par This diagram illustrates the simplified high-level architecture of the   SiteX platform, focusing on the clear separation of concerns between the user interface, backend services, and core data assets. It highlights the primary flow of data through the system during a typical user session.

\begin{itemize}
    \item \textbf{User Interface (Frontend):} This layer, depicted as a linear flowchart, represents the user's entire interactive journey. The process begins when the user defines their profile and requirements. This information is sent to the backend, which returns personalized map and listing data for the user to explore. After selecting a point for analysis, the relevant data is sent back to the backend. Finally, the frontend receives a complete analysis package to display on the detailed dashboard. This layer is responsible for all presentation and user interaction.

    \item \textbf{Backend Services:} This layer acts as the central brain of the application and is logically divided into two distinct components:
    \begin{itemize}
        \item \textbf{API Gateway:} The single, public-facing entry point for the backend. It handles all incoming requests from the User Interface (User Profile, Location for Analysis) and routes them to the engine. It also formats and sends back all responses (Map Data, Full Analysis Package).
        \item \textbf{Analysis \& Prediction Engine:} The internal powerhouse that performs all the heavy lifting. It receives tasks from the API Gateway and executes the core business logic: querying the database for geospatial data and loading the pre-trained XGBoost model to generate the full analysis.
    \end{itemize}

    \item \textbf{Data \& Model Assets:} These are the foundational pillars of the system's intelligence, accessed exclusively by the backend's Analysis \& Prediction Engine.
    \begin{itemize}
        \item \textbf{Relational Database (PostGIS):} Stores all the live, structured geospatial data, including competitor locations, demographic polygons, points of interest, and real estate listings.
        \item \textbf{Trained Model (XGBoost):} The static, pre-trained machine learning model file. It contains the predictive intelligence required to calculate the Personalized Suitability Score.
    \end{itemize}
\end{itemize}
% \pagebreak
% \setlength{\parindent}{15pt}
\section{Zero Context level Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/Level0ContextDiagram.png}
	\caption{0-level Context Diagram}
	\label{fig:Context-level-diagram}
\end{figure}
\noindent
\par This 0-level context diagram provides a high-level overview of the   SiteX platform, illustrating its boundaries and its interactions with external entities. It shows the primary data flows into and out of the system without detailing the internal processes.

\begin{description}
    \item[Central System:] The \textbf{SiteX} is the core process that transforms raw data and user requirements into actionable business insights.

	\item[External Entities:] External entities involve following: 
	\begin{itemize}
		\item \textbf{Entrepreneur:} The primary user who initiates the analysis and receives the final intelligence.
		\item \textbf{Project Admin:} The team responsible for curating and inputting verified ground-truth data into the system.
		\item \textbf{Public Geospatial Data Sources:} External sources like OpenStreetMap that provide foundational data (base maps, POIs, road networks).
		\item \textbf{Real Estate Data Sources:} External sources for property listings and pricing data, which are used for both the property discovery feature and for deriving demographic proxies.
		% \item \textbf{OpenRouteService API:} An external, third-party service that the system calls upon to perform the complex task of calculating realistic trade area isochrones.
	\end{itemize}

    \item[Primary Data Flows:] The diagram highlights the interactive, multi-step dialogue between the Entrepreneur and the system. The user first sends their \texttt{Business Profile}, receives a \texttt{Personalized Heatmap}, then sends a \texttt{Selected Location for Analysis} to receive the final \texttt{Detailed Analysis Dashboard \& Report}.
\end{description}
\pagebreak
\section{Level-1 Context Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/Level1ContextDiagram.png}
	\caption{Level-1 Context Diagram}
	\label{fig:level1-context-diagram}
\end{figure}
\noindent
The Level-1 Context Diagram provides a decomposed view of the SiteCortex system, expanding upon the 0-level context by detailing the system’s internal functional processes and their interactions with external entities and data stores.

\begin{itemize}
	\item \textbf{Core Processes:}
	The SiteCortex platform is divided into four primary internal processes:
	\begin{enumerate}
		\item \textbf{Handle API Requests}: Serves as the central coordination layer, managing incoming user requests, routing data flows, and aggregating responses.
		\item \textbf{Calculate Location Features}: Responsible for deriving spatial and contextual features such as competitor density, POI accessibility, and demographic indicators.
		\item \textbf{Predict Suitability Score}: Executes machine learning inference using the trained XGBoost model to compute the final suitability score.
		\item \textbf{Filter Property Listings}: Filters available “To-Let” properties based on user-defined requirements such as budget, area, and location constraints.
	\end{enumerate}

	\item \textbf{Data Stores:}
	Each process interacts with specialized databases optimized for specific data types:
	\begin{itemize}
		\item \textbf{Listings Database:} Stores property listings and rental information.
		\item \textbf{Competitor Database:} Contains data about existing businesses in the surrounding area.
		\item \textbf{Demographics Database:} Holds neighborhood-level population and socio-economic data.
		\item \textbf{POI Database:} Stores geospatial information about nearby amenities and landmarks.
		\item \textbf{Trained Model Repository:} Stores the pre-trained XGBoost model used for prediction.
	\end{itemize}

	\item \textbf{Data Flow Overview:}
	User requests originating from the UI Frontend enter the system through the API handler. Feature computation processes query the relevant databases and return calculated features. These features are then passed to the prediction process, which generates suitability scores and heatmap values. The filtered listings, scores, and visual analytics are consolidated and returned to the frontend for presentation.
\end{itemize}

This Level-1 Context Diagram emphasizes the modular, service-oriented architecture of SiteCortex, illustrating how independent yet interconnected components collaborate to transform raw geospatial data into personalized, decision-support intelligence for entrepreneurs.

\section{Workflow Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/WorkflowDiagram.png}
	\caption{Workflow Diagram}
	\label{fig:Workflow-diagram}
\end{figure}
\noindent
\par This diagram illustrates the two primary and distinct workflows that define the entire   SiteX project: the offline data science pipeline that builds the system's intelligence, and the online user journey that consumes it.

\begin{itemize}
    \item \textbf{Admin Workflow (Right Side):} This represents the foundational, \textbf{offline process} performed by the project team. It begins with collecting multi-layered data (geospatial, demographic, listings) and populating the relational database. This structured data is then fed into an ML Pipeline to engineer features and train the XGBoost model. The two critical outputs of this workflow are the system's core assets: the populated \textbf{Relational Database} and the final \textbf{Trained Model} file.

    \item \textbf{Business Owner's Workflow (Left Side):} This represents the \textbf{real-time, online journey} for the end-user. The process is initiated when the user defines their requirements. The backend uses these to filter listings and generate a personalized heatmap. After the user explores the map and selects a point, the backend performs the full analysis by calculating the score, gathering insights, and returning the complete package to the user's detailed dashboard.

    \item \textbf{The Bridge:} The diagram critically shows the connection between the two workflows. The assets created by the offline Admin Workflow are the essential inputs that power the live Business Owner Workflow. The \textbf{Relational Database} is actively queried for generating the heatmap and gathering competitor data, while the \textbf{Trained Model} is loaded on-demand to calculate the personalized score, seamlessly connecting the data science foundation to the interactive user application.
\end{itemize}
% \noindent
%     The development of the predictive site selection platform will follow a systematic methodology grounded in geospatial data analysis and supervised machine learning. The project is designed to transform raw geographic data into actionable business intelligence, providing a data-driven forecast of a new retail location's potential success. The workflow encompasses geospatial data aggregation, feature engineering, predictive model training, and deployment via an interactive web application.

\section{Sequence Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/SequenceDiagram.png}
	\caption{Sequence Diagram}
	\label{fig:sequence-diagram}
\end{figure}
\noindent
This sequence diagram illustrates the chronological interaction between the user, frontend interface, backend services, databases, and the machine learning model during a complete analysis cycle in the SiteCortex platform. It highlights how a user’s request is transformed into actionable location intelligence through coordinated backend operations.

\begin{itemize}
	\item \textbf{User \& Frontend Interaction:}
	The workflow begins when the user accesses the system through the UI Frontend. Initially, the frontend requests nearby Points of Interest (POIs) data to render an interactive base map. The user then enters a specific location and defines their business preferences, such as business type and constraints.

	\item \textbf{API Request Handling:}
	Upon submission, the UI sends an analysis request to the backend Handle API Requests service. This component acts as the orchestrator, managing communication between feature computation services, databases, and the prediction engine.

	\item \textbf{Feature Calculation Process:}
	The API service requests location-based features from the Calculate Location Features module. This module queries multiple data sources—including POI data, competitor information, and demographic datasets—from the database cluster. After aggregating and processing this information, it returns a structured feature vector representing the selected location.

	\item \textbf{Suitability Prediction:}
	The computed feature vector is forwarded to the Predict Suitability Score service. This service dynamically loads the pre-trained XGBoost model and performs inference to generate a personalized suitability score. In parallel, supporting spatial outputs such as heatmap intensity values are generated.

	\item \textbf{Result Delivery:}
	The predicted score and heatmap data are returned to the API service, which then sends the consolidated results back to the UI Frontend. Finally, the frontend displays the suitability score, visual heatmap, and detailed insights to the user, completing the interaction loop.
\end{itemize}

This sequence diagram clearly demonstrates how SiteCortex integrates real-time user inputs with geospatial analytics and machine learning inference to deliver instant, data-driven business location recommendations.
\section{Data Collection and Preprocessing}
\subsection{Geospatial Data Collection and Aggregation}

The project's foundation is built on a comprehensive multi-layered geographic dataset covering the commercial areas in Kathmandu. The data collection process evolved through several iterations to achieve efficiency and scale:

\begin{enumerate}
	\item \textbf{Manual Data Collection:} Initially, data was collected manually through field surveys and Google Maps exploration. However, this approach proved to be highly time-consuming and difficult to scale across the geographic area.
	
	\item \textbf{Google Maps Scraper Extension:} To improve efficiency, the \textbf{Chrome Google Maps Scraper extension} was utilized. However, this tool had significant limitations, extracting only approximately 30 entries per scraping session, making it impractical for large-scale data aggregation.
	
	\item \textbf{Apify Web Scraping Platform:} To overcome these limitations, \textbf{Apify}, a professional web scraping platform, was employed. This tool proved highly effective, enabling the collection of a comprehensive dataset exceeding \textbf{3,000 total data entries}, including all Points of Interest (POIs) and competitor businesses. Among these, the cafe category alone contributed \textbf{1,330 entries}.
\end{enumerate}

The collected data is organized into four primary layers within a PostGIS database:

\begin{itemize}
	\item \textbf{Competitors (Cafe Businesses):} Locations of all existing cafes sourced via Apify web scraping (1,330 entries), with precise geographic coordinates and metadata.
	\item \textbf{Points of Interest (POIs):} Locations of foot-traffic attractors including schools, temples, offices, hospitals, banks, and other public amenities (from the remaining 1,670+ entries). This layer identifies "people magnets" that influence foot traffic patterns.
    \item \textbf{Neighborhoods Layer:} GIS polygons defining commercial zones across Kathmandu, enriched with demographic proxies derived from POI density analysis and real estate data.
\end{itemize}

\subsection{POI Weight Calculation and Aggregation}

A critical preprocessing step involved systematic calculation of POI weights and category-level aggregation, performed by the data preprocessing pipeline (\texttt{master.py}) prior to model training. The methodology proceeded as follows:

\begin{enumerate}
	\item \textbf{Individual POI Weighting:} Each Point of Interest was assigned a weight reflecting its influence on foot traffic generation and commercial viability. These weights were derived by combining multiple components: base category weight, normalized rating scores, normalized review counts, and normalized weekly operating hours. These components are averaged to produce a unified weight for each POI.
	
	\item \textbf{POI Category Aggregation:} POIs were grouped into meaningful categories: \textbf{banks}, \textbf{temples}, \textbf{education institutions}, \textbf{health facilities}, and \textbf{other amenities}. For each category, both raw counts and weighted sums were calculated for all POIs within a one-kilometer radius of each cafe location. These values were then normalized across the dataset to enable fair comparison.
	
	\item \textbf{Cafe-Level Scoring:} Individual cafe characteristics (rating, review count, weekly hours, search rank) were normalized and combined to produce a cafe properties score. Additionally, a "nearby cafes" component was calculated by aggregating the property scores of other cafes within the analysis radius.
	
	\item \textbf{Composite Score Generation:} For each cafe in the dataset, a final \textbf{poi\_composite\_score} (the target variable for prediction) was calculated by combining normalized category components, weighted cafe properties, and nearby cafe influence. This continuous score (ranging from 0.558 to 3.868, mean: 2.097) represents the location's commercial viability based on its surrounding amenities and competitive environment.
\end{enumerate}

The output of this preprocessing pipeline is a structured dataset where each cafe location has pre-computed POI count features, POI weight features, and the composite score target variable. This processed dataset serves as the input for the machine learning training phase.

\subsection{Mathematical Formulas for Preprocessing Pipeline}

The following formulas describe the computations performed by the preprocessing pipeline (\texttt{master.py}) when producing per-POI weights, per-cafe POI metrics, and the final composite score. Note that these calculations are performed \textbf{prior to model training}, and the resulting pre-computed features are loaded by the training notebook.

\subsubsection{Per-POI Weight Calculation}
Per-POI combined weight (for POI $j$): available components for a POI (base weight, rating norm, reviews norm, weekly hours norm) are averaged. For $k$ available components $c_{jm}$:
$$
\text{combined}_j=\frac{1}{k}\sum_{m=1}^{k}c_{jm}
$$

\subsubsection{Distance Calculation}
Haversine distance (meters) between points with latitude/longitude $(\varphi_1,\lambda_1)$ and $(\varphi_2,\lambda_2)$ (in radians), with Earth radius $R=6{,}371{,}000\,$m:
$$
\Delta\varphi=\varphi_2-\varphi_1,\quad\Delta\lambda=\lambda_2-\lambda_1
$$
$$
a=\sin^2\left(\tfrac{\Delta\varphi}{2}\right)+\cos\varphi_1\cos\varphi_2\sin^2\left(\tfrac{\Delta\lambda}{2}\right)
$$
$$
d=2R\arctan2\big(\sqrt{a},\sqrt{1-a}\big)
$$

\subsubsection{Category-Level Aggregation and Normalization}
For each POI category $p$ and cafe location $i$, weights within the search radius (e.g., 1km) are summed and normalized:
$$
\text{norm\_}w_{i}^{(p)}=\frac{w_{i}^{(p)}}{\max_j w_{j}^{(p)}}\quad(\text{else }0)
$$

Category component with category-specific weight:
$$
\text{component}_{i}^{(p)}=\text{norm\_}w_{i}^{(p)}\times\text{category\_weight}_{p}
$$

\subsubsection{Cafe Property Score}
Cafe-level property components (rating, reviews, weekly hours, rank) are normalized individually (e.g., $\text{rating}/5$, $\log(1+\text{reviews})/\max\log(1+\text{reviews})$) and averaged:
$$
\text{cafe\_props}_i=\frac{1}{K}\sum_{m=1}^{K}p_{im}
$$
where $p_{im}$ are the available normalized property components.

\subsubsection{Nearby Cafes Component}
Sum cafe property scores for other cafes within the analysis radius $R_{\text{master}}$ (excluding the cafe itself):
$$
\text{nearby\_sum}_i=\sum_{j\ne i:\ d_{ij}\le R_{\text{master}}}\text{cafe\_props}_j
$$
Normalized nearby component:
$$
\text{nearby\_norm}_i=\frac{\text{nearby\_sum}_i}{\max_j\text{nearby\_sum}_j}\quad(\text{else }0)
$$

\subsubsection{Final Composite Score (Target Variable)}
The \texttt{poi\_composite\_score} combines all category components, cafe properties, and nearby cafe influence:
\begin{multline*}
\text{poi\_composite\_score}_i=\sum_{p}\text{component}_{i}^{(p)}+\text{PROPS\_WEIGHT}\times\text{cafe\_props}_i \\
+\text{NEIGHBOR\_WEIGHT}\times\text{nearby\_norm}_i
\end{multline*}

This composite score serves as the \textbf{target variable} (Y) for the supervised learning regression model. The trained model learns to predict this score for new locations based on their POI feature vectors.

\subsubsection{Individual Cafe Weight}
For reference, individual cafe weights (normalized cafe properties) are calculated as:
$$
\text{cafe\_individual\_score}_i=\text{cafe\_props}_i
$$
$$
\text{cafe\_weight}_i=\frac{\text{cafe\_individual\_score}_i}{\max_j\text{cafe\_individual\_score}_j}\quad(\text{else }0)
$$

\textbf{Note:} All these computations are performed during the preprocessing phase. The model training notebook loads the resulting dataset with pre-computed count features (e.g., \texttt{banks\_count\_1km}), weight features (e.g., \texttt{banks\_weight\_1km}), and the target variable \texttt{poi\_composite\_score}.



\subsection{Feature Engineering and Data Structuring}

The preprocessed POI weights and category aggregations are further refined through feature engineering using Python libraries such as \textbf{GeoPandas}, \textbf{Shapely}, and \textbf{scikit-learn}. The feature engineering process transforms raw geospatial data into a comprehensive feature set designed to capture location attractiveness.

\subsubsection{Raw Features}
For any given point on the map, the following base features are extracted:
\begin{itemize}
	\item \textbf{Geographic Coordinates:} Latitude (\texttt{lat}) and longitude (\texttt{lng}) values.
	\item \textbf{POI Count Features:} Raw counts of each POI category within 1km radius:
	\begin{itemize}
		\item \texttt{banks\_count\_1km}: Number of banking institutions
		\item \texttt{education\_count\_1km}: Number of educational facilities
		\item \texttt{health\_count\_1km}: Number of healthcare facilities
		\item \texttt{temples\_count\_1km}: Number of religious sites
		\item \texttt{other\_count\_1km}: Count of other amenities
	\end{itemize}
	\item \textbf{POI Weight Features:} Quality-adjusted weighted scores for each POI category:
	\begin{itemize}
		\item \texttt{banks\_weight\_1km}, \texttt{education\_weight\_1km}, \texttt{health\_weight\_1km}, \texttt{temples\_weight\_1km}, \texttt{other\_weight\_1km}
	\end{itemize}
\end{itemize}

\subsubsection{Engineered Features}
Advanced features are derived to capture complex spatial relationships:
\begin{itemize}
	\item \textbf{\texttt{total\_poi\_count\_1km}:} Total count of all POIs within 1km radius, representing overall area activity level.
	\item \textbf{Category Ratio Features:} Proportional distribution of each POI type:
	\begin{itemize}
		\item \texttt{bank\_ratio}: Proportion of banks among all POIs
		\item \texttt{education\_ratio}: Proportion of educational facilities
		\item \texttt{temple\_ratio}: Proportion of religious sites
		\item \texttt{health\_ratio}: Proportion of healthcare facilities
	\end{itemize}
	\item \textbf{\texttt{weighted\_POI\_strength}:} A composite metric combining all weighted POI scores to represent the overall quality-adjusted attractiveness of the location.
	\item \textbf{Target Variable (\texttt{poi\_composite\_score}):} A continuous value representing location commercial viability, derived from weighted POI aggregations and existing cafe performance data.
\end{itemize}

\subsubsection{Data Preprocessing and Handling}
\begin{itemize}
	\item \textbf{Missing Value Imputation:} Missing values are handled using median imputation to maintain data integrity without introducing bias.
	\item \textbf{Outlier Detection:} Statistical analysis and visualization techniques identify and handle outliers in the dataset.
	\item \textbf{Feature Scaling:} For baseline models requiring normalization, \texttt{StandardScaler} is applied to ensure features are on comparable scales.
	\item \textbf{Duplicate Handling:} Initial data inspection removes duplicate entries to ensure training data quality.
\end{itemize}

\subsubsection{Dataset Partitioning}
To prepare for model training and rigorous evaluation, the dataset is partitioned using a standard train-test split, following best practices for supervised learning model validation.

\subsection{Real-Time Location Scoring System}

The core of the application is a system that provides data-driven suitability predictions for any location in the study area. The prediction workflow operates as follows:

\begin{itemize}
    \item The user interacts with an \textbf{interactive map interface} displaying the study area.
	\item Upon selecting any point on the map, the frontend sends the geographic coordinates (latitude, longitude) to the backend API.
	\item The backend employs a \textbf{k-Nearest Neighbors (k=5) spatial query} against the pre-computed POI dataset to efficiently estimate all required features for that location:
	\begin{itemize}
		\item POI count features (e.g., \texttt{banks\_count\_1km}, \texttt{education\_count\_1km})
		\item POI weight features (e.g., \texttt{banks\_weight\_1km}, \texttt{education\_weight\_1km})
		\item Engineered ratio features (e.g., \texttt{bank\_ratio}, \texttt{education\_ratio})
		\item Aggregate strength metric (\texttt{weighted\_POI\_strength})
	\end{itemize}
	\item The computed feature vector is passed to the pre-trained XGBoost model for inference.
	\item The model returns a \textbf{Location Suitability Score} (continuous value in range 0.558-3.868).
	\item For heatmap generation, predictions are batched (1000 locations at a time) for computational efficiency.
	\item The system categorizes scores into risk levels: High Risk ($<$1.0), Medium Risk (1.0-2.0), Low Risk ($>$2.0).
\end{itemize}

\subsection{Supervised Learning Regression Model}

The project employs a \textbf{Supervised Learning} regression approach to predict location suitability. The model is trained on a labeled dataset of 1,330 existing cafe locations, each with pre-computed POI features and a known \texttt{poi\_composite\_score} target value.

The system follows the standard supervised regression framework:

\begin{table}[h!]
\centering
\begin{tabular}{|l|p{0.7\textwidth}|}
	\hline
	\textbf{ML Component}         & \textbf{Description} \\
	\hline
	\textbf{Input Features (X):}  & A 16-dimensional feature vector comprising: geographic coordinates (\texttt{lat}, \texttt{lng}), POI count features for 5 categories, POI weight features for 5 categories, \texttt{total\_poi\_count\_1km}, 4 ratio features (\texttt{bank\_ratio}, \texttt{education\_ratio}, \texttt{temple\_ratio}, \texttt{health\_ratio}), and \texttt{weighted\_POI\_strength}. \\
	\hline
	\textbf{Target Variable (Y):} & \texttt{poi\_composite\_score}: A continuous value (range 0.558-3.868, mean 2.097) representing location commercial viability based on surrounding amenities, cafe properties, and competitive environment. \\
	\hline
	\textbf{Model Architecture:}  & \texttt{xgb.XGBRegressor} with default hyperparameters - a gradient boosted decision tree ensemble chosen for superior performance on tabular geospatial data, built-in regularization, and ability to capture non-linear spatial relationships. \\
	\hline
	\textbf{Prediction Output:}   & A single continuous score (e.g., 2.35) representing the predicted \texttt{poi\_composite\_score} for a new location, indicating its commercial attractiveness. \\
	\hline
\end{tabular}
\end{table}

\subsubsection{Actual Feature Vector Structure}
The complete input feature vector for a given location (after feature engineering):
\begin{verbatim}
Feature Vector (16 dimensions):
[
  lat, lng,                           # Geographic coordinates
  banks_count_1km, education_count_1km, health_count_1km, 
  temples_count_1km, other_count_1km, # POI counts
  banks_weight_1km, education_weight_1km, health_weight_1km,
  temples_weight_1km, other_weight_1km, # POI weights
  total_poi_count_1km,                # Total POI count
  bank_ratio, education_ratio, temple_ratio, health_ratio, # Ratios
  weighted_POI_strength                # Aggregate strength
]

Example values for a high-potential location:
[27.7172, 85.3240, 12, 8, 5, 3, 15, 45.2, 32.1, 
 18.5, 12.3, 38.7, 43, 0.279, 0.186, 0.070, 0.116, 146.8]
\end{verbatim}

\subsection{Model Training and Evaluation Strategy}

The training process follows a rigorous methodology to ensure both accuracy and reliability, with comparative analysis between baseline and advanced models.

\subsubsection{Model Selection and Training}
\begin{enumerate}
	\item \textbf{Baseline Model (Linear Regression):} A linear regression model with standardized features serves as the baseline, providing a performance benchmark. The model is implemented using scikit-learn's \texttt{Pipeline} combining \texttt{StandardScaler} and \texttt{LinearRegression} to ensure proper feature normalization.
	
	\item \textbf{Primary Model (XGBoost):} The production model uses \texttt{xgb.XGBRegressor}, a gradient boosted decision tree algorithm chosen for its:
	\begin{itemize}
		\item Superior performance on tabular geospatial data
		\item Built-in regularization preventing overfitting
		\item Ability to capture non-linear relationships between POI features
		\item Scale invariance (no feature normalization required)
		\item Native support for missing values
	\end{itemize}
\end{enumerate}

\subsubsection{Evaluation Metrics}
Model performance is assessed on the held-out test set using comprehensive regression metrics:
\begin{itemize}
	\item \textbf{Mean Absolute Error (MAE):} Measures the average absolute difference between predicted and actual scores, providing an interpretable error metric in the same units as the target variable.
	\item \textbf{Root Mean Squared Error (RMSE):} Similar to MAE but penalizes larger errors more heavily, ensuring the model performs well across the entire distribution.
	\item \textbf{R-squared (R²):} Indicates the proportion of variance in the success score that is predictable from the input features, with values closer to 1.0 indicating better model fit.
	\item \textbf{Mean and Standard Deviation of Residuals:} Assess model bias and prediction consistency.
\end{itemize}

\subsubsection{Model Performance Results}
The XGBoost model demonstrates exceptional predictive performance on the test dataset:

\begin{table}[h!]
\centering
\begin{tabular}{|l|r|}
	\hline
	\textbf{Metric} & \textbf{Value} \\
	\hline
	\textbf{MAE} & 0.0908 points \\
	\textbf{RMSE} & 0.1176 points \\
	\textbf{R² Score} & 0.9822 (98.22\%) \\
	\textbf{Mean Residual} & 0.00191 \\
	\textbf{Std Residual} & 0.1178 \\
	\hline
\end{tabular}
\caption{XGBoost Model Performance Metrics}
\label{tab:model-performance}
\end{table}

The R² score of 0.9822 indicates that the model successfully explains 98.22\% of the variance in location suitability scores. The near-zero mean residual (0.00191) demonstrates that predictions are unbiased, while the low MAE (0.0908) confirms the model's practical accuracy for real-world deployment.

\subsubsection{Model Persistence and Deployment}
\begin{itemize}
	\item The trained XGBoost model is serialized using \texttt{joblib} and saved to the models directory as \texttt{xgb\_baseline.pkl}.
	\item Feature names are stored separately as \texttt{model\_features.pkl} to ensure consistency between training and production inference.
	\item The production prediction pipeline requires only latitude and longitude as input, automatically computing all engineered features using k-Nearest Neighbors (k=5) for efficient spatial feature estimation.
	\item Predictions are batched (batch size = 1000) for computational efficiency when generating heatmaps.
\end{itemize}

\subsubsection{Risk Assessment Framework}
The system translates raw suitability scores into actionable risk categories:
\begin{itemize}
	\item \textbf{High Risk:} Score $<$ 1.0 (location not recommended)
	\item \textbf{Medium Risk:} Score between 1.0 and 2.0 (proceed with caution)
	\item \textbf{Low Risk:} Score $>$ 2.0 (favorable location)
\end{itemize}

\subsection{Database Schema and Storage}

To support real-time feature calculation and the overall application logic, all aggregated geographic data is stored in a professional-grade relational database system.
\begin{itemize}
	\item \textbf{Database Technology:} \textbf{PostgreSQL} with the \textbf{PostGIS} extension.
	\item \textbf{Why PostGIS?} This extension transforms the database into a powerful geospatial data store, providing functions to perform high-performance geographic queries directly in the database (e.g., \texttt{ST\_DWithin}, \texttt{ST\_Distance}), which is extremely efficient for our use case.
\end{itemize}

The system is built on a four-table relational schema as described in the ER diagram:
\begin{description}
    \item[\texttt{neighborhoods}] Stores GIS polygons for different zones along with their derived demographic proxies (e.g., \texttt{avg\_rent\_psf}).
    \item[\texttt{businesses}] The ground-truth data of existing competitor cafes used for training the model, including location and calculated popularity scores.
    \item[\texttt{points\_of\_interest}] Locations that attract foot traffic, such as schools, temples, and offices.
    \item[\texttt{listings}] Dynamic data about available "To-Let" properties, including area, rent, and location.
\end{description}
