\chapter{Methodology}
\section{Block Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/BlockDiagram.png}
	\caption{Block Diagram}
	\label{fig:block-diagram}
\end{figure}
This block diagram illustrates the system architecture of \textbf{  SiteX}, detailing the interaction between the frontend user interface, the backend services, and the core data and model assets.

\begin{itemize}
    \item \textbf{User Interface (Frontend):} The user's journey begins here. They input their unique \textbf{Business Profile} (e.g., \textit{`Quiet Study Hub' vibe}, budget, rent) into the UI. The frontend is responsible for rendering the \textbf{Personalized Opportunity Zone Heatmap} and, upon user selection of a point, displaying the rich, multi-faceted \textbf{Detailed Analysis Dashboard}.

    \item \textbf{Backend Services (API Gateway):} Acting as the central nervous system, the FastAPI backend orchestrates the entire process. When it receives the user's profile, it tasks the \textbf{Property Listing Filter} to query the database for relevant "To-Let" properties. Simultaneously, its \textbf{Dynamic Feature Analyzer} queries the geospatial databases (Competitors, POIs, Demographics) to generate the data needed for the personalized heatmap. When a user requests a deep analysis, the Feature Analyzer gathers all features for that specific point and passes them to the \textbf{Predictor Service}.

    \item \textbf{Data \& Model Assets:} These are the foundational pillars of the system's intelligence.
    \begin{itemize}
        \item \textbf{The Database Cluster:} A PostgreSQL database with the PostGIS extension, containing separate, optimized tables for \texttt{Listings}, \texttt{Competitors} (Businesses), \texttt{Demographics} (Neighborhoods), and \texttt{Points of Interest}.
        \item \textbf{The Trained Model:} A pre-trained \textbf{XGBoost model file}. The Predictor Service loads this model to calculate the final, personalized \textbf{Suitability Score} for the user's specific business profile at the selected location.
    \end{itemize}
\end{itemize}
% \pagebreak
\section{Usecase Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/UsecaseDiagram.png}
	\caption{Usecase Diagram}
	\label{fig:Usecase-diagram}
\end{figure}\
\noindent
\par This diagram illustrates the simplified high-level architecture of the   SiteX platform, focusing on the clear separation of concerns between the user interface, backend services, and core data assets. It highlights the primary flow of data through the system during a typical user session.

\begin{itemize}
    \item \textbf{User Interface (Frontend):} This layer, depicted as a linear flowchart, represents the user's entire interactive journey. The process begins when the user defines their profile and requirements. This information is sent to the backend, which returns personalized map and listing data for the user to explore. After selecting a point for analysis, the relevant data is sent back to the backend. Finally, the frontend receives a complete analysis package to display on the detailed dashboard. This layer is responsible for all presentation and user interaction.

    \item \textbf{Backend Services:} This layer acts as the central brain of the application and is logically divided into two distinct components:
    \begin{itemize}
        \item \textbf{API Gateway:} The single, public-facing entry point for the backend. It handles all incoming requests from the User Interface (User Profile, Location for Analysis) and routes them to the engine. It also formats and sends back all responses (Map Data, Full Analysis Package).
        \item \textbf{Analysis \& Prediction Engine:} The internal powerhouse that performs all the heavy lifting. It receives tasks from the API Gateway and executes the core business logic: querying the database for geospatial data and loading the pre-trained XGBoost model to generate the full analysis.
    \end{itemize}

    \item \textbf{Data \& Model Assets:} These are the foundational pillars of the system's intelligence, accessed exclusively by the backend's Analysis \& Prediction Engine.
    \begin{itemize}
        \item \textbf{Relational Database (PostGIS):} Stores all the live, structured geospatial data, including competitor locations, demographic polygons, points of interest, and real estate listings.
        \item \textbf{Trained Model (XGBoost):} The static, pre-trained machine learning model file. It contains the predictive intelligence required to calculate the Personalized Suitability Score.
    \end{itemize}
\end{itemize}
% \pagebreak
% \setlength{\parindent}{15pt}
\section{Zero Context level Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/Level0ContextDiagram.png}
	\caption{0-level Context Diagram}
	\label{fig:Context-level-diagram}
\end{figure}
\noindent
\par This 0-level context diagram provides a high-level overview of the   SiteX platform, illustrating its boundaries and its interactions with external entities. It shows the primary data flows into and out of the system without detailing the internal processes.

\begin{description}
    \item[Central System:] The \textbf{SiteX} is the core process that transforms raw data and user requirements into actionable business insights.

	\item[External Entities:] External entities involve following: 
	\begin{itemize}
		\item \textbf{Entrepreneur:} The primary user who initiates the analysis and receives the final intelligence.
		\item \textbf{Project Admin:} The team responsible for curating and inputting verified ground-truth data into the system.
		\item \textbf{Public Geospatial Data Sources:} External sources like OpenStreetMap that provide foundational data (base maps, POIs, road networks).
		\item \textbf{Real Estate Data Sources:} External sources for property listings and pricing data, which are used for both the property discovery feature and for deriving demographic proxies.
		% \item \textbf{OpenRouteService API:} An external, third-party service that the system calls upon to perform the complex task of calculating realistic trade area isochrones.
	\end{itemize}

    \item[Primary Data Flows:] The diagram highlights the interactive, multi-step dialogue between the Entrepreneur and the system. The user first sends their \texttt{Business Profile}, receives a \texttt{Personalized Heatmap}, then sends a \texttt{Selected Location for Analysis} to receive the final \texttt{Detailed Analysis Dashboard \& Report}.
\end{description}
\pagebreak
\section{Level-1 Context Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/Level1ContextDiagram.png}
	\caption{Level-1 Context Diagram}
	\label{fig:level1-context-diagram}
\end{figure}
\noindent
The Level-1 Context Diagram provides a decomposed view of the SiteCortex system, expanding upon the 0-level context by detailing the system’s internal functional processes and their interactions with external entities and data stores.

\begin{itemize}
	\item \textbf{Core Processes:}
	The SiteCortex platform is divided into four primary internal processes:
	\begin{enumerate}
		\item \textbf{Handle API Requests}: Serves as the central coordination layer, managing incoming user requests, routing data flows, and aggregating responses.
		\item \textbf{Calculate Location Features}: Responsible for deriving spatial and contextual features such as competitor density, POI accessibility, and demographic indicators.
		\item \textbf{Predict Suitability Score}: Executes machine learning inference using the trained XGBoost model to compute the final suitability score.
		\item \textbf{Filter Property Listings}: Filters available “To-Let” properties based on user-defined requirements such as budget, area, and location constraints.
	\end{enumerate}

	\item \textbf{Data Stores:}
	Each process interacts with specialized databases optimized for specific data types:
	\begin{itemize}
		\item \textbf{Listings Database:} Stores property listings and rental information.
		\item \textbf{Competitor Database:} Contains data about existing businesses in the surrounding area.
		\item \textbf{Demographics Database:} Holds neighborhood-level population and socio-economic data.
		\item \textbf{POI Database:} Stores geospatial information about nearby amenities and landmarks.
		\item \textbf{Trained Model Repository:} Stores the pre-trained XGBoost model used for prediction.
	\end{itemize}

	\item \textbf{Data Flow Overview:}
	User requests originating from the UI Frontend enter the system through the API handler. Feature computation processes query the relevant databases and return calculated features. These features are then passed to the prediction process, which generates suitability scores and heatmap values. The filtered listings, scores, and visual analytics are consolidated and returned to the frontend for presentation.
\end{itemize}

This Level-1 Context Diagram emphasizes the modular, service-oriented architecture of SiteCortex, illustrating how independent yet interconnected components collaborate to transform raw geospatial data into personalized, decision-support intelligence for entrepreneurs.

\section{Workflow Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/WorkflowDiagram.png}
	\caption{Workflow Diagram}
	\label{fig:Workflow-diagram}
\end{figure}
\noindent
\par This diagram illustrates the two primary and distinct workflows that define the entire   SiteX project: the offline data science pipeline that builds the system's intelligence, and the online user journey that consumes it.

\begin{itemize}
    \item \textbf{Admin Workflow (Right Side):} This represents the foundational, \textbf{offline process} performed by the project team. It begins with collecting multi-layered data (geospatial, demographic, listings) and populating the relational database. This structured data is then fed into an ML Pipeline to engineer features and train the XGBoost model. The two critical outputs of this workflow are the system's core assets: the populated \textbf{Relational Database} and the final \textbf{Trained Model} file.

    \item \textbf{Business Owner's Workflow (Left Side):} This represents the \textbf{real-time, online journey} for the end-user. The process is initiated when the user defines their requirements. The backend uses these to filter listings and generate a personalized heatmap. After the user explores the map and selects a point, the backend performs the full analysis by calculating the score, gathering insights, and returning the complete package to the user's detailed dashboard.

    \item \textbf{The Bridge:} The diagram critically shows the connection between the two workflows. The assets created by the offline Admin Workflow are the essential inputs that power the live Business Owner Workflow. The \textbf{Relational Database} is actively queried for generating the heatmap and gathering competitor data, while the \textbf{Trained Model} is loaded on-demand to calculate the personalized score, seamlessly connecting the data science foundation to the interactive user application.
\end{itemize}
% \noindent
%     The development of the predictive site selection platform will follow a systematic methodology grounded in geospatial data analysis and supervised machine learning. The project is designed to transform raw geographic data into actionable business intelligence, providing a data-driven forecast of a new retail location's potential success. The workflow encompasses geospatial data aggregation, feature engineering, predictive model training, and deployment via an interactive web application.

\section{Sequence Diagram}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{img/SequenceDiagram.png}
	\caption{Sequence Diagram}
	\label{fig:sequence-diagram}
\end{figure}
\noindent
This sequence diagram illustrates the chronological interaction between the user, frontend interface, backend services, databases, and the machine learning model during a complete analysis cycle in the SiteCortex platform. It highlights how a user’s request is transformed into actionable location intelligence through coordinated backend operations.

\begin{itemize}
	\item \textbf{User \& Frontend Interaction:}
	The workflow begins when the user accesses the system through the UI Frontend. Initially, the frontend requests nearby Points of Interest (POIs) data to render an interactive base map. The user then enters a specific location and defines their business preferences, such as business type and constraints.

	\item \textbf{API Request Handling:}
	Upon submission, the UI sends an analysis request to the backend Handle API Requests service. This component acts as the orchestrator, managing communication between feature computation services, databases, and the prediction engine.

	\item \textbf{Feature Calculation Process:}
	The API service requests location-based features from the Calculate Location Features module. This module queries multiple data sources—including POI data, competitor information, and demographic datasets—from the database cluster. After aggregating and processing this information, it returns a structured feature vector representing the selected location.

	\item \textbf{Suitability Prediction:}
	The computed feature vector is forwarded to the Predict Suitability Score service. This service dynamically loads the pre-trained XGBoost model and performs inference to generate a personalized suitability score. In parallel, supporting spatial outputs such as heatmap intensity values are generated.

	\item \textbf{Result Delivery:}
	The predicted score and heatmap data are returned to the API service, which then sends the consolidated results back to the UI Frontend. Finally, the frontend displays the suitability score, visual heatmap, and detailed insights to the user, completing the interaction loop.
\end{itemize}

This sequence diagram clearly demonstrates how SiteCortex integrates real-time user inputs with geospatial analytics and machine learning inference to deliver instant, data-driven business location recommendations.
\section{Data Collection and Preprocessing}
\subsection{Geospatial Data Collection and Aggregation}

The project's foundation is built on a comprehensive multi-layered geographic dataset covering the commercial areas in Kathmandu. The data collection process evolved through several iterations to achieve efficiency and scale:

\begin{enumerate}
	\item \textbf{Manual Data Collection:} Initially, data was collected manually through field surveys and Google Maps exploration. However, this approach proved to be highly time-consuming and difficult to scale across the geographic area.
	
	\item \textbf{Google Maps Scraper Extension:} To improve efficiency, the \textbf{Chrome Google Maps Scraper extension} was utilized. However, this tool had significant limitations, extracting only approximately 30 entries per scraping session, making it impractical for large-scale data aggregation.
	
	\item \textbf{Apify Web Scraping Platform:} To overcome these limitations, \textbf{Apify}, a professional web scraping platform, was employed. This tool proved highly effective, enabling the collection of a comprehensive dataset exceeding \textbf{3,000 total data entries}, including all Points of Interest (POIs) and competitor businesses. Among these, the cafe category alone contributed \textbf{1,330 entries}.
\end{enumerate}

The collected data is organized into four primary layers within a PostGIS database:

\begin{itemize}
	\item \textbf{Competitors (Cafe Businesses):} Locations of all existing cafes sourced via Apify web scraping (1,330 entries), with precise geographic coordinates and metadata.
	\item \textbf{Points of Interest (POIs):} Locations of foot-traffic attractors including schools, temples, offices, hospitals, banks, and other public amenities (from the remaining 1,670+ entries). This layer identifies "people magnets" that influence foot traffic patterns.
    \item \textbf{Neighborhoods Layer:} GIS polygons defining commercial zones across Kathmandu, enriched with demographic proxies derived from POI density analysis and real estate data.
\end{itemize}

\subsection{POI Weight Calculation and Aggregation}

A critical preprocessing step involved systematic calculation of POI weights and category-level aggregation. The methodology proceeded as follows:

\begin{enumerate}
	\item \textbf{Individual POI Weighting:} Each Point of Interest was assigned a weight reflecting its influence on foot traffic generation and commercial viability. These weights were derived based on POI category importance and proximity effects.
	
	\item \textbf{POI Category Aggregation:} POIs were grouped into meaningful categories: \textbf{banks}, \textbf{temples}, \textbf{education institutions}, \textbf{health facilities}, \textbf{other amenities}, and \textbf{competitor cafes}. For each category, an \textbf{aggregate POI score} was calculated by summing the weighted influence of all POIs in that category.
	
	\item \textbf{Location-based Scoring:} For each cafe in the dataset, a final \textbf{composite suitability score} was calculated based on the aggregate POI scores of all categories within a one-kilometer radius buffer. This score represents the cumulative "attractiveness" of the immediate surroundings as determined by proximity to various amenities and competition.
\end{enumerate}

This multi-step weighting and aggregation process converts raw POI location data into meaningful predictive features that capture the commercial dynamics of each location.

\subsection{Mathematical definitions and formulas}

The following formulas summarise the computations performed by `master.py` when producing per-POI weights, per-cafe POI metrics, and the final composite score.

Per-POI combined weight (for POI $j$): components available for a POI (base weight, rating norm, reviews norm, weekly hours norm) are averaged. For $k$ available components $c_{jm}$:
$$
	ext{combined}_j=\frac{1}{k}\sum_{m=1}^{k}c_{jm}
$$

Haversine distance (meters) between points with latitude/longitude $(\varphi_1,\lambda_1)$ and $(\varphi_2,\lambda_2)$ (in radians), with Earth radius $R=6{,}371{,}000\,$m:
$$
\Delta\varphi=\varphi_2-\varphi_1,\quad\Delta\lambda=\lambda_2-\lambda_1
$$
$$
a=\sin^2\left(\tfrac{\Delta\varphi}{2}\right)+\cos\varphi_1\cos\varphi_2\sin^2\left(\tfrac{\Delta\lambda}{2}\right)
$$
$$
d=2R\arctan2\big(\sqrt{a},\sqrt{1-a}\big)
$$

% % Per-cafe POI metrics (for category $p$ and cafe $i$):
% % - Let $w_{i}^{(p)}$ be the sum of `_computed_weight` for category $p$ within the radius (e.g. 1km).
% % - Normalise by the category maximum (when $\max(w^{(p)})>0$):
$$
	ext{norm\_}w_{i}^{(p)}=\frac{w_{i}^{(p)}}{\max_j w_{j}^{(p)}}\quad(\text{else }0)
$$
- Category component:
$$
	ext{component}_{i}^{(p)}=\text{norm\_}w_{i}^{(p)}\times\text{category\_weight}_{p}
$$

Cafe-level property components (rating, reviews, weekly hours, rank) are normalised individually (e.g. $\text{rating}/5$, $\log(1+\text{reviews})/\max\log(1+\text{reviews})$, weekly hours normalized to default) and averaged to produce the cafe properties score:
$$
	ext{cafe\_props}_i=\frac{1}{K}\sum_{m=1}^{K}p_{im}
$$
where $p_{im}$ are the available property components.

Nearby-cafes component: sum cafe property scores for other cafes within the master radius $R_{\text{master}}$ (exclude the cafe itself):
$$
	ext{nearby\_sum}_i=\sum_{j\ne i:\ d_{ij}\le R_{\text{master}}}\text{cafe\_props}_j
$$
Normalise:
$$
	ext{nearby\_norm}_i=\frac{\text{nearby\_sum}_i}{\max_j\text{nearby\_sum}_j}\quad(\text{else }0)
$$

Final composite POI score (summed over all category components plus cafe props and nearby component):
$$
	ext{poi\_composite\_score}_i=\sum_{p}\text{component}_{i}^{(p)}+\text{PROPS\_WEIGHT}\times\text{props}_i+\text{NEIGHBOR\_WEIGHT}\times\text{nearby\_norm}_i
$$

Individual cafe weight (normalised cafe properties):
$$
	ext{cafe\_individual\_score}_i=\text{cafe\_props}_i,\qquad
	ext{cafe\_weight}_i=\frac{\text{cafe\_individual\_score}_i}{\max_j\text{cafe\_individual\_score}_j}\quad(\text{else }0)
$$



\subsection{Feature Engineering and Data Structuring}

The preprocessed POI weights and category aggregations are further refined through feature engineering using Python libraries such as \textbf{GeoPandas} and \textbf{Shapely}. For any given point on the map, a rich set of predictive features is calculated:

\begin{itemize}
	\item \textbf{Competitor Density:} The number of other cafes (weighted) within a one-kilometre radius.
	\item \textbf{POI Proximity Features:} Aggregate weighted scores for each POI category (banks, temples, education, health, other) within one kilometre.
    \item \textbf{Neighborhood-based Features:} Demographic context inherited from the neighborhood polygon in which the location falls.
	\item \textbf{Composite Suitability Score (Target Variable):} A continuous value representing the location's commercial viability, derived from the weighted POI aggregations and existing cafe performance data.
\end{itemize}

To prepare for model training and evaluation, the dataset is partitioned using a standard train-test-validation split: 70\% for training, 15\% for validation (hyperparameter tuning), and 15\% for final testing on unseen data.

\subsection{Personalized Predictive Modeling System}

The core of the application is a system that provides personalized, real-time predictions for new, hypothetical locations. The user interaction is designed to be intuitive and user-centric:

\begin{itemize}
    \item A user begins by defining their specific \textbf{business profile}, including strategy (e.g., 'Quiet Study Hub', 'Tourist Hotspot'), budget, and space requirements.
	\item The backend uses this profile to generate a \textbf{personalized opportunity heatmap} and filter relevant property listings.
	\item The user selects any point on the interactive map for deep analysis.
	\item The frontend sends the coordinates and the user's profile to the backend API.
	\item The backend, in real-time, calculates all engineered features for that specific point by querying the geographic database.
	\item These features are fed into the pre-trained machine learning model, which returns a \textbf{Personalized Suitability Score}.
	\item The system also provides an explainable AI \textbf{Prediction Breakdown} (powered by SHAP analysis), showing which features contributed most positively or negatively to the score, offering actionable insights.
\end{itemize}

\subsection{Predictive Engine via Supervised Learning}

The project utilizes a \textbf{Supervised Learning} regression model. This approach is ideal as we are training a model to predict a specific, known continuous value (the Popularity Score) based on a labeled dataset of existing, successful and unsuccessful cafes.

The system is modeled using the standard supervised regression framework:

\begin{table}[h!]
\centering
\begin{tabular}{|l|p{0.7\textwidth}|}
	\hline
	\textbf{ML Component}         & \textbf{Description} \\
	\hline
	\textbf{Input Features (X):}  & A vector of numerical values representing the engineered features, including competitor density, proximity to POIs, visibility scores, and inherited demographic proxies from the neighborhood. \\
	\hline
	\textbf{Target Variable (Y):} & The continuous "Popularity Score" that the model learns to predict, acting as a proxy for general business success. \\
	\hline
	\textbf{Model Architecture:}  & A gradient boosted decision tree model, specifically \textbf{XGBoost}, chosen for its high accuracy, performance, and interpretability on tabular data. \\
	\hline
	\textbf{Prediction Output:}   & A single, continuous value (e.g., 8.2) representing the \textbf{Personalized Suitability Score} for a new location, tailored to the user's business profile. \\
	\hline
\end{tabular}
\end{table}

\subsubsection{Example Input Feature Vector}
An example input vector fed to the model for a new location would be enriched with demographic data:
\begin{verbatim}
[ 2, 4, 250, 80, 3, 4, 150, 0.75 ] 
// Represents [competitors_100m, competitors_300m, 
// dist_to_school, dist_to_bus_stop, road_type_score, 
// visibility_score, neighborhood_avg_rent_psf, 
// neighborhood_student_density]
\end{verbatim}

\subsection{Model Training and Evaluation Strategy}

The training process is designed to ensure the model is both accurate and reliable.
\begin{itemize}
	\item A baseline model (e.g., Linear Regression) will be established to measure the performance uplift provided by our chosen model.
	\item The primary model, \textbf{XGBoost}, will be trained on the 70\% training dataset.
	\item Model performance will be assessed on the test set using standard regression metrics:
	      \begin{itemize}
		      \item \textbf{Mean Absolute Error (MAE):} Measures the average absolute difference between predicted and actual scores.
		      \item \textbf{Root Mean Squared Error (RMSE):} Similar to MAE but penalizes larger errors more heavily.
		      \item \textbf{R-squared (R²):} Indicates the proportion of the variance in the success score that is predictable from the input features.
	      \end{itemize}
\end{itemize}

\subsection{Database Schema and Storage}

To support real-time feature calculation and the overall application logic, all aggregated geographic data is stored in a professional-grade relational database system.
\begin{itemize}
	\item \textbf{Database Technology:} \textbf{PostgreSQL} with the \textbf{PostGIS} extension.
	\item \textbf{Why PostGIS?} This extension transforms the database into a powerful geospatial data store, providing functions to perform high-performance geographic queries directly in the database (e.g., \texttt{ST\_DWithin}, \texttt{ST\_Distance}), which is extremely efficient for our use case.
\end{itemize}

The system is built on a four-table relational schema as described in the ER diagram:
\begin{description}
    \item[\texttt{neighborhoods}] Stores GIS polygons for different zones along with their derived demographic proxies (e.g., \texttt{avg\_rent\_psf}).
    \item[\texttt{businesses}] The ground-truth data of existing competitor cafes used for training the model, including location and calculated popularity scores.
    \item[\texttt{points\_of\_interest}] Locations that attract foot traffic, such as schools, temples, and offices.
    \item[\texttt{listings}] Dynamic data about available "To-Let" properties, including area, rent, and location.
\end{description}
