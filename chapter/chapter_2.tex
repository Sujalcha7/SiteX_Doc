\chapter{Literature Review}
\section{The Huff Gravity Model of Retail Location}
David Huff’s seminal paper laid the foundations for empirically grounded retail site evaluation, establishing a rigorous mathematical template still cited by everyday analysts and academics alike \citep{huff1963mathematical}. The model posits that the probability a consumer will patronize a particular store hinges on a competitive interplay between the outlet’s intrinsic “magnetism” (factored through variables such as preferred brand mix, size of draw-in anchors, and overall service reputation) and an “impedance” function that compresses transactional distance, journey time, and parking or toll costs into a scalar deterrent. The original model distills this logic into a compact formula that allows analysts to chart potential catchment districts by estimating patronage probabilities across candidate centers. Despite catalyzing much subsequent research, the model is circumscribed by its architecture, which squeezes increasingly intricate consumer dynamics into a linear constraint. Variables that exert decisive influence locational overlap with rivals, sight-line quality, micro-demographic clusteringmerely serve as implicit residuals. Our undertaking therefore pivots to a data-centric paradigm that extracts functional relationships directly from wide-screen transaction, mobility, and sensor data, dispensing with customary equations in favour of interpretable models refined by cross-validated out-of-sample predictive power.

\section{Applying Machine Learning to Retail Site Selection}
Moving beyond static formulas, researchers began applying machine learning to the site selection problem to better capture its complexity. In a notable study, Kuo et. al. demonstrated the superiority of AI-based approaches over traditional methods \citep{kuo2002application}. The authors used a decision tree model to predict the sales volume of new convenience stores. They integrated a wide range of parameters, including demographic data (population, income) and location-specific variables (number of nearby competitors, proximity to schools and offices). Their findings showed that the machine learning model could uncover complex, interactive relationships between these variables that were invisible to traditional statistical models. The model achieved significantly higher predictive accuracy, proving that AI could provide a more robust and reliable framework for site selection. This paper validates our core methodology of using an ensemble of data points to train a predictive model.

\section{The Role of Demographics in Site Selection}
Academic and industry research consistently confirms that location alone is insufficient for predicting retail success; the demographic and psychographic profile of the surrounding population is equally critical. Studies have shown that factors such as average household income, population density, age distribution, and lifestyle indicators are strong predictors of a store's sales potential and long-term viability. For instance, a premium brand may fail in a low-income area despite high foot traffic, while a budget-friendly outlet might thrive. Our project incorporates this principle by creating demographic proxies for wealth and lifestyle, ensuring our model recommends locations that are not just busy, but are busy with the *right* target audience for a given business profile.


\section{XGBoost: A Gradient Boosting Framework}
For projects involving predictive modeling on structured, tabular datasuch as oursthe choice of algorithm is critical. Gradient Boosting frameworks are widely regarded as the state-of-the-art for such tasks. Our primary choice is \textbf{XGBoost}, an optimized and scalable implementation of gradient boosted decision trees presented by Chen \& Guestrin \citep{chen2016xgboost}. Gradient boosting is an ensemble technique that builds a strong predictive model by sequentially adding weaker models, where each new model corrects the errors of its predecessor. The power of XGBoost lies in its performance, regularization techniques to prevent overfitting, and its inherent ability to be interpreted using methods like SHAP, which is crucial for our "Prediction Breakdown" feature.

While XGBoost is a robust and proven choice, we acknowledge the dynamic nature of machine learning. Our methodology includes the evaluation of contemporary alternatives to ensure the best possible performance. These include:
\begin{itemize}
    \item \textbf{LightGBM:} A Microsoft-developed framework known for its high speed and efficiency, often outperforming XGBoost on large datasets.
    \item \textbf{CatBoost:} A framework that excels at handling categorical features automatically, which could simplify our data preprocessing pipeline for variables like \verb|road type|.
\end{itemize}
This comparative approach ensures we select the most effective algorithm for the unique characteristics of our dataset. 

\section{Industry Standard: Location Intelligence Platforms}
The academic pursuit of site selection has materialized into a significant industry known as Location Intelligence. Commercial platforms like Placer.ai and CARTO represent the current state-of-the-art in this field \citep{placerai2022}. These platforms have moved beyond census data and leverage large-scale, anonymized mobile location data to derive highly accurate metrics. They provide dashboards that visualize real-time foot traffic patterns, customer demographics (psychographics), trade area analysis, and competitor performance. For instance, Placer.ai can show how many people visit a competitor's cafe, where they came from, what time of day is busiest, and what other stores they frequent. While building a system with access to such proprietary mobile data is beyond our project's scope, studying these platforms provides a crucial benchmark. Our project aims to emulate the analytical power of these systems by creating our own unique dataset and using AI to derive similar predictive insights.

\section{GIS-based Feature Engineering for Machine Learning}
A critical challenge in any geospatial prediction task is converting raw map data into meaningful features that a machine learning model can understand. A study by Al-Ruzouq et. al. provides a clear framework for this process, which is often termed "geospatial feature engineering" \citep{alruzouq2019gis}. The authors demonstrate the use of Geographic Information Systems (GIS) tools to systematically generate predictive variables. Their methodology involved using GIS to calculate buffer zones around potential sites, count the number of competitors within those zones, measure the distance to the nearest major road or point of interest (POI), and determine the density of the surrounding road network. These engineered features were then used to train a machine learning model for predicting land suitability. This paper is directly relevant to our project, as it outlines the exact process we will follow: using a GIS tool (QGIS) to transform our raw geographic data into a structured set of features (competitor density, proximity to 'people magnets', etc.) that will form the input for our XGBoost model.