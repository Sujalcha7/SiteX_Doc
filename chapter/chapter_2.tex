\chapter{Literature Review}
\section{The Huff Gravity Model of Retail Location}
In his seminal paper, David Huff presented a probabilistic model for retail site selection that became a cornerstone of location analysis \citep{huff1963mathematical}. The Huff Gravity Model posits that the probability of a consumer patronizing a specific retail location is a function of two main factors: the "attractiveness" of the store (e.g., its size, variety of goods) and the "friction" of distance (the time and cost to travel to it). The model provides a mathematical formula to estimate the trade area of a potential store by calculating these probabilities for surrounding population centers. While foundational and still influential, the model is inherently limited. It relies on a relatively simple formula and struggles to incorporate the vast number of complex, non-linear variables that influence modern consumer behavior, such as competitor density, visibility, and local demographic nuances. Our project aims to overcome these limitations by using a data-driven approach rather than a predefined formula.

\section{Applying Machine Learning to Retail Site Selection}
Moving beyond static formulas, researchers began applying machine learning to the site selection problem to better capture its complexity. In a notable study, Kuo et. al. demonstrated the superiority of AI-based approaches over traditional methods \citep{kuo2002application}. The authors used a decision tree model to predict the sales volume of new convenience stores. They integrated a wide range of parameters, including demographic data (population, income) and location-specific variables (number of nearby competitors, proximity to schools and offices). Their findings showed that the machine learning model could uncover complex, interactive relationships between these variables that were invisible to traditional statistical models. The model achieved significantly higher predictive accuracy, proving that AI could provide a more robust and reliable framework for site selection. This paper validates our core methodology of using an ensemble of data points to train a predictive model.

\section{XGBoost: A Gradient Boosting Framework}
For projects involving predictive modeling on structured, tabular data—such as ours—the choice of algorithm is critical. Chen \& Guestrin presented XGBoost, an optimized and scalable implementation of gradient boosted decision trees that has become a dominant force in applied machine learning \citep{chen2016xgboost}. Gradient boosting is an ensemble technique that builds a strong predictive model by sequentially adding weaker models, where each new model corrects the errors of its predecessor. The power of XGBoost lies in its significant optimizations. It employs advanced regularization techniques (L1 and L2) to prevent overfitting, is designed for parallel processing to drastically reduce training time, and can gracefully handle missing data. For our project, which will involve a dataset with many diverse features (demographic, geographic, competitive), XGBoost offers the high performance, accuracy, and robustness required to build a reliable predictive engine.

\section{Industry Standard: Location Intelligence Platforms}
The academic pursuit of site selection has materialized into a significant industry known as Location Intelligence. Commercial platforms like Placer.ai and CARTO represent the current state-of-the-art in this field \citep{placerai2022}. These platforms have moved beyond census data and leverage large-scale, anonymized mobile location data to derive highly accurate metrics. They provide dashboards that visualize real-time foot traffic patterns, customer demographics (psychographics), trade area analysis, and competitor performance. For instance, Placer.ai can show how many people visit a competitor's cafe, where they came from, what time of day is busiest, and what other stores they frequent. While building a system with access to such proprietary mobile data is beyond our project's scope, studying these platforms provides a crucial benchmark. Our project aims to emulate the analytical power of these systems by creating our own unique dataset and using AI to derive similar predictive insights.

\section{GIS-based Feature Engineering for Machine Learning}
A critical challenge in any geospatial prediction task is converting raw map data into meaningful features that a machine learning model can understand. A study by Al-Ruzouq et. al. provides a clear framework for this process, which is often termed "geospatial feature engineering" \citep{alruzouq2019gis}. The authors demonstrate the use of Geographic Information Systems (GIS) tools to systematically generate predictive variables. Their methodology involved using GIS to calculate buffer zones around potential sites, count the number of competitors within those zones, measure the distance to the nearest major road or point of interest (POI), and determine the density of the surrounding road network. These engineered features were then used to train a machine learning model for predicting land suitability. This paper is directly relevant to our project, as it outlines the exact process we will follow: using a GIS tool (QGIS) to transform our raw geographic data into a structured set of features (competitor density, proximity to 'people magnets', etc.) that will form the input for our XGBoost model.